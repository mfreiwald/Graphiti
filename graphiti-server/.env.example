# Graphiti REST Server Configuration
# Copy this file to .env and fill in your values

# ==================== OpenAI Configuration ====================
# Required for LLM operations and embeddings
OPENAI_API_KEY=sk-your-key-here

# Optional: Use a different OpenAI-compatible API
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Specify models
MODEL_NAME=gpt-5-mini
SMALL_MODEL_NAME=gpt-5-nano
EMBEDDER_MODEL_NAME=text-embedding-3-large

# LLM Temperature (0.0-2.0, lower = more deterministic)
LLM_TEMPERATURE=1.0


# ==================== Azure OpenAI Configuration (Optional) ====================
# Use Azure OpenAI instead of OpenAI API

# Azure OpenAI LLM Configuration
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
# AZURE_OPENAI_API_VERSION=2024-02-15-preview
# AZURE_OPENAI_USE_MANAGED_IDENTITY=false

# Azure OpenAI Embedding Configuration
# AZURE_OPENAI_EMBEDDING_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small
# AZURE_OPENAI_EMBEDDING_API_VERSION=2024-02-15-preview
# AZURE_OPENAI_EMBEDDING_API_KEY=your-key-here  # Optional, defaults to OPENAI_API_KEY


# ==================== Neo4j Configuration ====================
# Required for graph storage

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# For Docker Compose setup, use:
# NEO4J_URI=bolt://neo4j:7687
# NEO4J_PASSWORD=demodemo


# ==================== Server Configuration ====================
# Host and port for the REST server

SERVER_HOST=0.0.0.0
SERVER_PORT=8000


# ==================== Performance Configuration ====================
# Semaphore limit controls concurrent Graphiti operations
# Decrease if hitting rate limits (429 errors)
# Increase if you have high rate limits

SEMAPHORE_LIMIT=10


# ==================== Telemetry ====================
# Set to false to disable anonymous telemetry

# GRAPHITI_TELEMETRY_ENABLED=true
